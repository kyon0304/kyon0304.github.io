<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ostep on kyon's wonderland with ❤️</title><link>https://kyon.life/tags/ostep/</link><description>Recent content in ostep on kyon's wonderland with ❤️</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 01 Sep 2021 18:37:33 +0800</lastBuildDate><atom:link href="https://kyon.life/tags/ostep/index.xml" rel="self" type="application/rss+xml"/><item><title>操作系统导论学习笔记（六）</title><link>https://kyon.life/post/ostep-6/</link><pubDate>Wed, 01 Sep 2021 18:37:33 +0800</pubDate><guid>https://kyon.life/post/ostep-6/</guid><description>地址转换的二次尝试：分段 打破三个 简化假设 中的第一条：「用户的地址空间必须连续地放在物理内存中」。在 MMU 中引入不止一个基址和界限寄存器，而是给地址空间内的每个逻辑段（segment）一对。在典型的地址空间里，有 3 个逻辑不同的段：代码段、栈和堆。
段寄存器的值
分段机制 使得操作系统可以将不同的段放到不同物理内存区域，避免了地址空间中未使用的部分占用内存。
分段的地址空间示意图
在物理内存中放置段
计算地址转换时，需要先减去地址空间中的偏移，比如要访问堆的虚拟地址为 4200, 需要先减去地址空间中 4k 的偏移量，然后再根据基址寄存器中得到实际地址：4200-4096+34816=34920
分段的引入的问题：虚拟地址属于哪个段 将地址空间分段映射后，除了需要多对基址和界限寄存器，还需要辨识要访问的地址属于哪个段
显式方式：
虚拟地址 4200 的二进制形式
在虚拟地址中，前 2 位用于标识属于哪个段，上图中 01 表示属于堆，后面是段内偏移量，0000 0110 1000（即十六进制 0x068 或十进制 104 ），然后和基址寄存器中的地址相加得到物理地址 104+34816=34920
有的实现中，会把堆和栈当作一个段，这样就只有两个段，只需要一个标识位。
隐式方式：
硬件通过地址产生方式来判断属于哪个段，如果地址由程序计数器（ PC 指针）产生，那么无疑是代码段，如果基于栈或基址指针，那么这个虚拟地址在栈段，其他情况则来自堆段。
分段的引入的问题：栈段增长方向 栈的增长方向和代码段以及堆不一样，是反向增长的，比如栈被分配到 28k 地址，大小为 2k, 那么结束地址是 26k，因此需要硬件支持：新增一位标识地址是否反向增长。
分段的引入的问题：共享 随着分段机制的改进，内存使用效率纳入考虑，如果内存可以跨地址空间共享，可以明显提升使用率，对于代码段这种只读形式的内存，完全可以由操作系统同时映射到多个地址空间中，而不会担心隔离遭到破坏。
如果要支持内存共享，则需要区分内存的访问方式（只读、读写、可执行等），段寄存器再追加一位保护位，如下图所示。
段寄存器的值（有保护位）
另外，地址转换算法也需要改进，除了判断是否越界，还需要检查特定访问是否允许：如果用户试图写入只读区域，或从非执行段执行指令，硬件会触发异常，让操作系统来处理出错进程。
分段：小结 代码段、栈、堆这种划分段的方式是 粗粒度（coarse grained），相对的，有的操作系统支持将地址空间划分为成千上万的段，这种是 细粒度（fine grained）。</description></item><item><title>操作系统导论学习笔记（五）</title><link>https://kyon.life/post/ostep-5/</link><pubDate>Wed, 01 Sep 2021 14:57:48 +0800</pubDate><guid>https://kyon.life/post/ostep-5/</guid><description>内存使用的演化 最开始，操作系统只是库函数，和用户程序各占有一部分物理内存
接着，为了更有效地共享机器，多道程序开始流行，但很快，人们希望提高系统的交互性，时分系统开始流行，需要让多个程序共享内存，一种方式是类似上下文切换，将内存中的数据移动到磁盘中保存，以及从磁盘加载恢复，但是这个方式的问题是太慢了，尤其是程序占用内存越来越大的情况下，另一种方式是多个程序的内存同时驻留，CPU 选择某个进程执行，这种方式带来的问题是如何隔离及保护内存，毕竟不希望其他进程访问甚至修改当前进程的内存。
基于此，操作系统提供了易于使用的物理内存抽象，这个抽象叫做 地址空间 address space，是运行程序看到的系统中的内存。理解这个抽象，是理解虚拟内存的关键。
地址空间 一个进程的 地址空间 包含运行的程序的所有内存状态，比如代码需要加载到内存中，需要栈空间保存函数调用信息，为局部变量、参数、函数返回地址开辟空间，堆管理用户动态申请的空间，还有其他东西，比如静态变量等，这些都是地址空间的一部分。
不过为了简化讨论，我们假设只有代码、栈、堆这 3 部分。
地址空间简单示例
代码是固定的，程序运行期间不会发生变化，我们把它放在起始 0-1KB 部分
堆和栈在程序运行期间都有可能增长或缩小，比如用户申请内存、调用函数等情况，堆放在代码段后面向下增长，栈在地址空间底端向上增长。
以上是一种约定俗称，不是强制（比如当多个线程在地址空间中共存时）。
这里的地址空间，是操作系统提供给程序的抽象，而不是实际的物理内存地址，而是加载在任意的物理地址。
操作系统将程序看到的地址空间，转换为实际的物理地址，并从物理内存中获取内容，这是虚拟化内存中的关键。
虚拟内存目标 关键问题：操作系统如何在单一的物理内存上为多个运行的进程（所有进程共享内存）构建一个私有的、很大的地址空间抽象？
当操作系统这样做时，我们就说操作系统在 虚拟化内存 virtualizing memory
虚拟内存的 3 个目标
透明：应用程序不会察觉虚拟内存的存在，这些工作由操作系统和硬件在幕后完成，站在应用程序的角度，和直接使用物理内存无异 高效：时间上（不会因为虚拟内存而使应用程序运行变慢）和空间上（不会需要太多额外内存来支持虚拟内存实现），因此操作系统需要硬件支持来达到高效虚拟化内存的目的（比如 TLB） 保护：当一个进程加载、存储或执行指令时，不应该以任何形式访问或修改其他程序或操作系统本身的内存（即它地址空间之外的任何内存）。每个进程都应该在自己独立的环境中运行，避免其他出错或恶意进程的影响 虚拟内存所需 API 关键问题：如何分配和管理内存
在 UNIX/C 程序中，理解如何分配和管理内存是构建健壮和可靠软件的重要基础。通常使用哪些接口？需要避免哪些错误？
内存分配 malloc() 在运行一个 C 程序的时候，会分配两种内存
栈内存 由编译器自动分配，隐式管理 进入函数时，编译器自动在栈上开辟内存，退出函数后，自动释放 堆内存 所有的申请和释放都由程序员显式完成 void *malloc(size_t size); 存活时间由程序员决定 内存释放 free() 申请内存是内存管理中简单的部分，复杂并且容易出错的部分是释放内存 free(void* x);</description></item><item><title>操作系统导论学习笔记（三）</title><link>https://kyon.life/post/ostep-3/</link><pubDate>Wed, 01 Sep 2021 14:39:48 +0800</pubDate><guid>https://kyon.life/post/ostep-3/</guid><description>只有少量的物理 CPU，操作系统如何提供有几乎无限 CPU 可用的假象？
操作系统通过虚拟化(Virutalization) CPU 来提供这种假象。通过让每个程序只运行一个时间片，然后切换到其他进程，操作系统提供了存在多个虚拟 CPU 的假象。这就是时分共享(time sharing) CPU 共享技术。
潜在的开销就是性能降低，因为如果 CPU 必须共享，每个进程的运行就会变慢。
要实现 CPU 的虚拟化，操作系统需要提供底层 机制（mechanism） 和高层 策略（policy） 的支持：
机制是一些低级方法或协议，比如上下文切换（context switch），让操作系统可以停止运行当前程序，并在给定的 CPU 上运行另一个程序。 策略是操作系统作出某种决定的算法，比如调度策略（scheduling policy）决定当前 CPU 运行一组待运行程序中的哪一个。 分离机制和策略：机制为 how 提供答案，策略为 which 提供答案，将两者分开可以轻松地替换策略，而不必重新考虑机制。这是一种通用的软件设计原则：模块化。
关键问题：如何高效、可控地虚拟化 CPU
操作系统必须以高性能的方式虚拟化 CPU，同时保持对系统的控制。为此，操作系统会巧妙地利用硬件的支持。
受限的直接执行 基本技巧：受限的直接执行 LDE Limited Direct Execution
「直接执行」是指程序直接运行在 CPU 上
「受限」的体现之一，是 CPU 执行模式区分「用户模式 user mode」 和 「内核模式 kernel mode」，用户模式下，程序执行是受限制的，比如不能执行特权操作的（比如访问磁盘 I/O）。
LDE 协议的具体实现方式：
操作系统启动时，内核使用特权指令设置陷阱表 (trap table)，告知硬件接收到特定指令时，到哪里寻找需要执行的程序。 用户程序运行在用户模式下，发出系统调用后，硬件检测到变化，保存当前程序的状态（比如寄存器入栈），转为内核模式，查询陷阱表，然后跳转到对应的内核程序处执行。内核执行完程序后，调用返回用户模式指令，硬件恢复寄存器，转为用户模式，回到应用程序中继续执行。 系统调用类似过程调用，但隐藏在系统接口里的实现，是著名的陷阱指令。为了仔细遵循与内核一致的调用约定（例如将参数放在知名位置），库函数的系统调用部分是用汇编语言手动实现的。
LDE 协议的两阶段实现时间线</description></item><item><title>操作系统导论学习笔记（二）</title><link>https://kyon.life/post/ostep-2/</link><pubDate>Wed, 01 Sep 2021 13:46:35 +0800</pubDate><guid>https://kyon.life/post/ostep-2/</guid><description>进程 操作系统为正在运行的程序提供的抽象，就是进程。
操作系统也是一种应用程序，会使用数据结构保存进程相关的信息。比如保存正在运行的进程的一些附加信息，保存就绪进程列表，跟踪阻塞进程的信息，以便在合适的时机进行唤醒。进程列表这种数据结构，有时也会被称为 程序控制块 PCB Program Control Block
在任何时刻，都可以清点进程在读取和修改什么内容，机器的哪部分会对进程造成影响，我们称之为进程的机器状态（machine state）
机器状态包括：
内存：程序执行的指令和读取及修改的数据，进程可访问的内存称为进程的地址空间 通用寄存器 一些特殊寄存器 PC 指针：程序正在执行的指令 栈指针 stack pointer，帧指针 frame pointer 用于管理函数参数栈、局部变量和返回地址 I/O 信息： 程序访问的持久化存储设备 现代系统进程都会提供的 API
创建：程序变成进程的过程 销毁：如果程序不肯自己退出，操作系统提供了接口让用户结束进程 等待：有时等待进程停止运行是有用的 其他控制：除等待和销毁外的其他控制接口，比如暂停执行和恢复执行 状态：查看进程状态 操作系统创建进程
从磁盘加载代码和静态数据（比如初始化变量）到内存中 尽早加载 eagerly load 惰性加载 lazily load 只加载执行到的片段，需要内存分页和交换机制支持 分配内存，提供给程序的运行时栈使用，也可能会使用参数（argc, argv）初始化栈 也可能会给程序分配堆内存 其他初始化任务，特别是 I/O 相关的 在 Unix 中，所有进程都默认有 3 个打开的文件描述符：标准输入、标准输出和错误输出 启动程序：通过跳转到 main() 例程，操作系统将 CPU 的控制权交到新创建的进程中，从而程序开始执行 进程的 3 种状态及相互转换</description></item><item><title>操作系统导论学习笔记（一）</title><link>https://kyon.life/post/ostep-1/</link><pubDate>Tue, 31 Aug 2021 22:45:12 +0800</pubDate><guid>https://kyon.life/post/ostep-1/</guid><description>为了让程序运行变得更容易，操作系统出现了。操作系统完成的事情包括，允许多个程序同时运行、让程序共享内存、让程序能够与设备交互等。操作系统负责确保系统既易于使用又正确高效的运行。操作系统通过虚拟化（virtualization）做到这一点，操作系统将物理资源（如 CPU、内存或磁盘）转换为更通用、更强大且更易于使用的虚拟形式。因此，我们有时将操作系统称为虚拟机(virtual machine)
设计和实现操作系统的目标 提供高性能，尽可能降低操作系统的性能开销 minimize the overhead 在应用程序之间以及操作系统和应用系统之间提供保护 protection 让进程间彼此隔离是实现保护的关键 isolation 保证高可靠性 reliability 能源效率 energy efficiency, 安全性 security, 移动性 mobility 操作系统的 3 个“简单”部分 虚拟化 virtualizing 持久性 persistence 并行 concurrency 虚拟化 virtualizing 虚拟化又分为虚拟化 CPU 和虚拟化内存。
虚拟化 CPU
在硬件的一些帮助下，操作系统负责提供这种假象（illusion），即系统拥有非常多的虚拟CPU的假象。将单个CPU（或其中一小部分）转换为看似无限数量的CPU，从而让许多程序看似同时运行，这就是所谓的虚拟化CPU（virtualizing the CPU）
虚拟化内存
每个进程访问自己的私有虚拟地址空间（virtual address space）（有时称为地址空间，address space），操作系统以某种方式映射到机器的物理内存上。一个正在运行的程序中的内存引用不会影响其他进程（或操作系统本身）的地址空间。对于正在运行的程序，它完全拥有自己的物理内存。但实际情况是，物理内存是由操作系统管理的共享资源。
持久化 persistence 内存 DRAM 中的数据是易失的，如果断电或系统崩溃，内存中的数据都会丢失。因此我们需要硬件和软件支持来持久地存储数据。
硬件以某种输入/输出设备（Input/Output, I/O）的形式出现。
操作系统中管理磁盘的软件通常称为文件系统（file system）。因此它负责以可靠和高效的方式，将用户创建的任何文件（file）存储在系统的磁盘上。
关键问题：如何持久地存储数据
文件系统是操作系统的一部分，负责管理持久的数据。持久性需要哪些技术才能正确地实现？需要哪些机制和策略才能高性能地实现？面对硬件和软件故障，可靠性如何实现？
并行 concurrency 关键问题：如何构建正确的并发程序
如果同一个内存空间中有很多并发执行的线程，如何构建一个正确工作的程序？操作系统需要什么原语？硬件应该提供哪些机制？我们如何利用它们来解决并发问题？
操作系统的简单历史 早期操作系统：只是一些库 引入保护机制：借助硬件的帮助，区分用户模式和内核模式 多道程序 multiprogram: 提高 CPU 执行效率，避免 I/O 处理拖慢 CPU 执行。出现 Unix 摩登时代：出现个人计算机，遗憾的是，对于操作系统来说，个人计算机起初代表了一次巨大的倒退，因为早期的系统忘记了（或从未知道）小型机时代的经验教训。幸运的是，经过一段时间的苦难后，小型计算机操作系统的老功能开始进入台式机。</description></item></channel></rss>